{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from fuzzywuzzy import fuzz\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation_stopword = [\".\", '\"', \",\", \"?\", \"!\", \"/\", \"'\", \"-\", \"_\", \";\", \":\", \"&\", ',\"', '\",', \")\", \"(\", \"Golden\", \"Globes\", \"@\", \"GoldenGlobes\", \"I\", \"we\", \"http\", \"://\", \"/\", \"co\", \"Hollywood\", \"Hooray\"]\n",
    "humor_keywords = ['haha', 'lol','hh','233','funny','joke','hilarious','comedian', 'best joke', 'hysterical']\n",
    "stopwords = nltk.corpus.stopwords.words('english') + punctuation_stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweets_freq_dict = {}\n",
    "def remove_retweet_prefix(line):\n",
    "    # find 'RT @abc: ' where abc's length is arbitrary\n",
    "    pattern = re.compile(r'\\bRT @([\\w\\'/]*)\\b: ')\n",
    "\n",
    "    match = re.search(pattern, line)\n",
    "    if match:\n",
    "        # store corresponding retweet without 'RT @' prefix\n",
    "        string = match.group()[4:]\n",
    "        if string in retweets_freq_dict:\n",
    "            retweets_freq_dict[string] += 1\n",
    "        else:\n",
    "            retweets_freq_dict[string] = 1\n",
    "\n",
    "    return re.sub(pattern, ' ', line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_freq_dict = {}\n",
    "def remove_hashtag(line):\n",
    "    pattern = re.compile(r'#([\\w\\'/]*)\\b')\n",
    "    matches = re.findall(pattern, line)\n",
    "    if matches:\n",
    "        # store corresponding hashtag\n",
    "        for match in matches:\n",
    "            if match in hashtag_freq_dict:\n",
    "                hashtag_freq_dict[match] += 1\n",
    "            else:\n",
    "                hashtag_freq_dict[match] = 1\n",
    "\n",
    "    line = re.sub(pattern, ' ', line)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "at_freq_dict = {}\n",
    "def remove_at(line):\n",
    "    pattern = re.compile(r'@([\\w\\'/]*)\\b')\n",
    "    matches = re.findall(pattern, line)\n",
    "    if matches:\n",
    "        # store corresponding hashtag\n",
    "        for match in matches:\n",
    "            if match in at_freq_dict:\n",
    "                at_freq_dict[match] += 1\n",
    "            else:\n",
    "                at_freq_dict[match] = 1\n",
    "\n",
    "    line = re.sub(pattern, ' ', line)\n",
    "    return line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanse(line):\n",
    "    # replace everything to ' ' except whitespace, alphanumeric character, apostrophe, hashtag, @\n",
    "    return re.sub(r'[^\\w\\s\\'#@]', ' ', line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_apostrophe(text):\n",
    "    # remove_apostrophe\n",
    "    if text.endswith(\"'s\"):\n",
    "        return text[:-2].strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(line):\n",
    "    pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\\b')\n",
    "    matches = re.findall(pattern, line)\n",
    "    for match in matches:\n",
    "        line = re.sub(match, ' ', line)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_entities(data):\n",
    "    entities = list(nlp(data).ents)\n",
    "    tags = {}\n",
    "    for entity in entities:\n",
    "        if entity not in tags:\n",
    "            tags[' '.join(t.orth_ for t in entity).strip()]=[entity.label_]\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isHumor(text):\n",
    "    for x in humor_keywords:\n",
    "        if text.find(x) != -1:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_entity(data,verbose=False):\n",
    "    entity_freq_dict = {}\n",
    "\n",
    "    num = 0\n",
    "    for tweet in data:\n",
    "        # remove_retweet_prefix\n",
    "        line = remove_retweet_prefix(tweet)\n",
    "        # remove hashtag\n",
    "        line = remove_hashtag(line)\n",
    "        # remove @...\n",
    "        line = remove_at(line)\n",
    "        # remove url\n",
    "        line = remove_url(line)\n",
    "        # remove punctuations\n",
    "        line = cleanse(line)\n",
    "\n",
    "        match = isHumor(line)\n",
    "        if match:\n",
    "            tags = identify_entities(line)\n",
    "\n",
    "            #if verbose:\n",
    "                # print the first 10 occurrences\n",
    "                #if num < 10:\n",
    "                    #print(tweet)\n",
    "                    #print(line)\n",
    "                    #print(tags)\n",
    "                    #print()\n",
    "\n",
    "            for entity in tags.keys():\n",
    "                entity = remove_apostrophe(entity)\n",
    "                if len(entity) > 1:\n",
    "                    if entity not in entity_freq_dict:\n",
    "                        entity_freq_dict[\n",
    "                            entity] = 1  # tried adding more weights to 'PERSON' tags but results are not good\n",
    "                    else:\n",
    "                        entity_freq_dict[entity] += 1\n",
    "            num += 1\n",
    "\n",
    "    print('num of matches:', num)\n",
    "    return entity_freq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of matches: 6458\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_json(path_or_buf='gg2013.json')\n",
    "data = df['text']\n",
    "entity_freq_dict = find_entity(data,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "top_10 = sorted(entity_freq_dict.items(), key=lambda pair: pair[1], reverse=True)[:10]\n",
    "names = [pair[0] for pair in top_10]\n",
    "golden_globes = [name for name in names if fuzz.ratio(name.lower(), 'golden globes') > 50]\n",
    "pprint.pprint(golden_globes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we also consider dropping all lower cases examples or examples that contain digit(s), which are not names\n",
    "def filter_names(entity_freq_dict,pair_list):\n",
    "    filtered_results = []\n",
    "    for pair in pair_list:\n",
    "        string = ''.join(pair[0].split())\n",
    "        if not all(char.islower() for char in string) and not any(char.isdigit() for char in string):\n",
    "            filtered_results.append(pair)\n",
    "        else:\n",
    "            if pair[0] in entity_freq_dict:\n",
    "                del entity_freq_dict[pair[0]]\n",
    "    return filtered_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in golden_globes:\n",
    "    if name in entity_freq_dict:\n",
    "        del entity_freq_dict[name]\n",
    "top_results = sorted(entity_freq_dict.items(), key=lambda pair: pair[1], reverse=True)[:10]\n",
    "top_results = filter_names(entity_freq_dict,top_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Amy Poehler', 'Amy']\n",
      "['Tina Fey', 'Tina']\n",
      "['Tina', 'Tina Fey']\n",
      "['Amy', 'Amy Poehler']\n",
      "['Will Ferrell']\n",
      "['Kristen Wiig']\n",
      "['Taylor Swift']\n",
      "['Tonight']\n",
      "['Congratulations Lena Dunham']\n",
      "\n",
      "names clusters to merge:\n",
      "[['Tina', 'Tina Fey'], ['Amy', 'Amy Poehler']]\n"
     ]
    }
   ],
   "source": [
    "names = [pair[0] for pair in top_results]\n",
    "names_clusters = []\n",
    "\n",
    "for name in names:\n",
    "    # each name starts as a cluster\n",
    "    cluster = [name]\n",
    "    names_to_reduce = names[:]\n",
    "    names_to_reduce.remove(name)\n",
    "\n",
    "    # one vs. all comparisons\n",
    "    for i in names_to_reduce:\n",
    "        ratio = fuzz.ratio(name.lower(), i.lower())\n",
    "        # if similarity is larger than 75 or one name is contained in the other name\n",
    "        if ratio > 75 or re.search(name, i, flags=re.IGNORECASE) or re.search(i, name, flags=re.IGNORECASE):\n",
    "            cluster.append(i)\n",
    "\n",
    "    # if multiple names are identified in one cluster\n",
    "    if len(cluster) > 1:\n",
    "        names_clusters.append(cluster)\n",
    "\n",
    "    print(cluster)\n",
    "\n",
    "# find names clusters that should merge\n",
    "\n",
    "# sort clusters\n",
    "names_clusters.sort()\n",
    "# sort within each cluster\n",
    "names_clusters = ['|'.join(sorted(cluster)) for cluster in names_clusters]\n",
    "# remove overlaps\n",
    "names_clusters_reduced = [line.split('|') for line in list(set(names_clusters))]\n",
    "# sort by length from shortest to longest (merge from the shortest)\n",
    "names_clusters_reduced.sort(key=len)\n",
    "print('\\nnames clusters to merge:')\n",
    "pprint.pprint(names_clusters_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted frequency of an entity is defined by its frequency multiplied by its string length\n",
    "def weighted_freq(element):\n",
    "    return entity_freq_dict[element] * len(element)\n",
    "\n",
    "e = entity_freq_dict.copy()\n",
    "for cluster in names_clusters_reduced:\n",
    "    # select the longest entity name\n",
    "    selected_entity_name = max(cluster, key=weighted_freq)\n",
    "    cluster.remove(selected_entity_name)\n",
    "    # for names to be merged to the selected entity name\n",
    "    for name in cluster:\n",
    "        # if not deleted in previous cases, cumulate frequencies to the selected entity\n",
    "        if name in e and selected_entity_name in e:\n",
    "            e[selected_entity_name] += e[name]\n",
    "            del e[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Amy Poehler', 1918), ('Tina Fey', 1691), ('Will Ferrell', 236)]\n"
     ]
    }
   ],
   "source": [
    "h1 = sorted(e.items(), key=lambda pair: pair[1], reverse=True)[:3]\n",
    "print(h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"Humor\": [\"Amy Poehler\", \"Tina Fey\"]}'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top 2 inferences for humors\n",
    "best_host_prediction = [name[0] for name in top_5][:2]\n",
    "json.dumps({'Humor': best_host_prediction})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHumor(year):\n",
    "    df = pd.read_json(path_or_buf='gg'+str(year)+'.json')\n",
    "    data = df['text']\n",
    "    entity_freq_dict = find_entity(data,verbose=True)\n",
    "    top_10 = sorted(entity_freq_dict.items(), key=lambda pair: pair[1], reverse=True)[:10]\n",
    "\n",
    "    names = [pair[0] for pair in top_10]\n",
    "    golden_globes = [name for name in names if fuzz.ratio(name.lower(), 'golden globes') > 50]\n",
    "    for name in golden_globes:\n",
    "        if name in entity_freq_dict:\n",
    "            del entity_freq_dict[name]\n",
    "    top_results = sorted(entity_freq_dict.items(), key=lambda pair: pair[1], reverse=True)[:10]\n",
    "    top_results = filter_names(entity_freq_dict,top_results)\n",
    "\n",
    "    names = [pair[0] for pair in top_results]\n",
    "    names_clusters = []\n",
    "\n",
    "    for name in names:\n",
    "        # each name starts as a cluster\n",
    "        cluster = [name]\n",
    "        names_to_reduce = names[:]\n",
    "        names_to_reduce.remove(name)\n",
    "\n",
    "        # one vs. all comparisons\n",
    "        for i in names_to_reduce:\n",
    "            ratio = fuzz.ratio(name.lower(), i.lower())\n",
    "            # if similarity is larger than 75 or one name is contained in the other name\n",
    "            if ratio > 75 or re.search(name, i, flags=re.IGNORECASE) or re.search(i, name, flags=re.IGNORECASE):\n",
    "                cluster.append(i)\n",
    "\n",
    "        # if multiple names are identified in one cluster\n",
    "        if len(cluster) > 1:\n",
    "            names_clusters.append(cluster)\n",
    "\n",
    "        print(cluster)\n",
    "\n",
    "    # find names clusters that should merge\n",
    "\n",
    "    # sort clusters\n",
    "    names_clusters.sort()\n",
    "    # sort within each cluster\n",
    "    names_clusters = ['|'.join(sorted(cluster)) for cluster in names_clusters]\n",
    "    # remove overlaps\n",
    "    names_clusters_reduced = [line.split('|') for line in list(set(names_clusters))]\n",
    "    # sort by length from shortest to longest (merge from the shortest)\n",
    "    names_clusters_reduced.sort(key=len)\n",
    "\n",
    "    # weighted frequency of an entity is defined by its frequency multiplied by its string length\n",
    "    def weighted_freq(element):\n",
    "        return entity_freq_dict[element] * len(element)\n",
    "\n",
    "    e = entity_freq_dict.copy()\n",
    "    for cluster in names_clusters_reduced:\n",
    "        # select the longest entity name\n",
    "        selected_entity_name = max(cluster, key=weighted_freq)\n",
    "        cluster.remove(selected_entity_name)\n",
    "        # for names to be merged to the selected entity name\n",
    "        for name in cluster:\n",
    "            # if not deleted in previous cases, cumulate frequencies to the selected entity\n",
    "            if name in e and selected_entity_name in e:\n",
    "                e[selected_entity_name] += e[name]\n",
    "                del e[name]\n",
    "\n",
    "    top_3 = sorted(e.items(), key=lambda pair: pair[1], reverse=True)[:3]\n",
    "\n",
    "    # top 2 inferences for hosts\n",
    "    best_host_prediction = [name[0] for name in top_5][:2]\n",
    "    json.dumps({'Humor': best_host_prediction})\n",
    "\n",
    "    return top_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best joke in 2013 is said by:\n",
      "Amy Poehler\n",
      "Tina Fey\n",
      "Will Ferrell\n"
     ]
    }
   ],
   "source": [
    "print(\"The best joke in 2013 is said by:\")\n",
    "for item in h1:\n",
    "    print(item[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of matches: 40196\n",
      "['Amy', 'Amy Poehler']\n",
      "['Tina', 'Tina Fey']\n",
      "['Cosby', 'Bill Cosby']\n",
      "['Jeremy Renner']\n",
      "['Bill Cosby', 'Cosby']\n",
      "['Amy Poehler', 'Amy']\n",
      "['Tina Fey', 'Tina']\n",
      "['North Korea']\n",
      "['Oprah']\n",
      "The best joke in 2015 is said by:\n",
      "Bill Cosby\n",
      "Amy Poehler\n",
      "Tina\n"
     ]
    }
   ],
   "source": [
    "h2 = getHumor(2015)\n",
    "print(\"The best joke in 2015 is said by:\")\n",
    "for item in h2:\n",
    "    print(item[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
