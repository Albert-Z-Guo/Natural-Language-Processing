{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**determine who was**\n",
    "* best dressed\n",
    "* worst dressed \n",
    "* most controversial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(path_or_buf='gg2013.json')\n",
    "df.head(10)\n",
    "\n",
    "data = df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def cleanse(line):\n",
    "    # replace everything to ' ' except whitespace, alphanumeric character, apostrophe, hashtag, @\n",
    "    return re.sub(r'[^\\w\\s\\'#@]', ' ', line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('JLo', 'PROPN'), (\"'s\", 'PART'), ('dress', 'NOUN'), (' ', 'SPACE'), ('#', 'SYM'), ('eredcarpet', 'NOUN'), ('#', 'SYM'), ('GoldenGlobes', 'PROPN')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "# python -m spacy download en\n",
    "nlp = spacy.load('en')\n",
    "doc = nlp(cleanse(data[0]))\n",
    "print([(w.text, w.pos_) for w in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweets_freq_dict = {}\n",
    "\n",
    "def remove_retweet_prefix(line):\n",
    "    # find 'RT @abc: ' where abc's length is arbitrary\n",
    "    pattern = re.compile(r'\\bRT @([\\w\\'/]*)\\b: ') \n",
    "   \n",
    "    match = re.search(pattern, line)\n",
    "    if match:\n",
    "        # store corresponding retweet without 'RT @' prefix\n",
    "        string = match.group()[4:]\n",
    "        if string in retweets_freq_dict:\n",
    "            retweets_freq_dict[string] += 1\n",
    "        else:\n",
    "            retweets_freq_dict[string] = 1\n",
    "        \n",
    "    return re.sub(pattern, ' ', line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tina\n"
     ]
    }
   ],
   "source": [
    "def remove_apostrophe(text):\n",
    "    # remove_apostrophe\n",
    "    if text.endswith(\"'s\"):\n",
    "        return text[:-2].strip()\n",
    "    return text\n",
    "\n",
    "# test\n",
    "print(remove_apostrophe(\"Tina 's\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_freq_dict = {}\n",
    "\n",
    "def remove_hashtag(line):\n",
    "    pattern = re.compile(r'#([\\w\\'/]*)\\b')\n",
    "    matches = re.findall(pattern, line)\n",
    "    if matches:\n",
    "        # store corresponding hashtag\n",
    "        for match in matches:\n",
    "            if match in hashtag_freq_dict:\n",
    "                hashtag_freq_dict[match] += 1\n",
    "            else:\n",
    "                hashtag_freq_dict[match] = 1\n",
    "        \n",
    "    line = re.sub(pattern, ' ', line)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "def print_top(n, num):\n",
    "    sorted_nominees = sorted(n.items(), key=lambda e: e[1], reverse=True)\n",
    "    pprint.pprint(sorted_nominees[0:num])\n",
    "#     names = [pair[0] for pair in sorted_nominees]\n",
    "#     pprint.pprint(names[0:num])\n",
    "    print('list length:', len(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "at_freq_dict = {}\n",
    "\n",
    "def remove_at(line):\n",
    "    pattern = re.compile(r'@([\\w\\'/]*)\\b')\n",
    "    matches = re.findall(pattern, line)\n",
    "    if matches:\n",
    "        # store corresponding hashtag\n",
    "        for match in matches:\n",
    "            if match in at_freq_dict:\n",
    "                at_freq_dict[match] += 1\n",
    "            else:\n",
    "                at_freq_dict[match] = 1\n",
    "        \n",
    "    line = re.sub(pattern, ' ', line)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tina\n"
     ]
    }
   ],
   "source": [
    "def remove_apostrophe(text):\n",
    "    # remove_apostrophe\n",
    "    if text.endswith(\"'s\"):\n",
    "        return text[:-2].strip()\n",
    "    return text\n",
    "\n",
    "# test\n",
    "print(remove_apostrophe(\"Tina 's\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_freq_dict = {}\n",
    "\n",
    "def remove_hashtag(line):\n",
    "    pattern = re.compile(r'#([\\w\\'/]*)\\b')\n",
    "    matches = re.findall(pattern, line)\n",
    "    if matches:\n",
    "        # store corresponding hashtag\n",
    "        for match in matches:\n",
    "            if match in hashtag_freq_dict:\n",
    "                hashtag_freq_dict[match] += 1\n",
    "            else:\n",
    "                hashtag_freq_dict[match] = 1\n",
    "        \n",
    "    line = re.sub(pattern, ' ', line)\n",
    "    return line\n",
    "\n",
    "def remove_url(line):\n",
    "    pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\\b')\n",
    "    matches = re.findall(pattern, line)\n",
    "    for match in matches:\n",
    "        line = re.sub(match, ' ', line)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6000000000000001"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# pip3 install TextBlob\n",
    "\n",
    "def get_polarity(line):\n",
    "    blob = TextBlob(line)\n",
    "    for sentence in blob.sentences:\n",
    "#         print(sentence.sentiment.polarity)\n",
    "        return float(sentence.sentiment.polarity)\n",
    "    \n",
    "line = \"Jennifer Lopez's dress is jaw droppingly amazing #GoldenGlobes #redcarpet\"\n",
    "get_polarity(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 52s, sys: 19.6 s, total: 3min 12s\n",
      "Wall time: 49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from fuzzywuzzy import fuzz\n",
    "from textblob import TextBlob\n",
    "\n",
    "def find_names():\n",
    "    pattern = re.compile(\"(dress)|(wear)|(outfit)|(suit)|(cloth)\", re.IGNORECASE)\n",
    "    # pattern1 = re.compile(\"(\\saward\\s)|(\\sbest\\s)\", re.IGNORECASE)\n",
    "    neg_dict = {}\n",
    "    pos_dict = {}\n",
    "\n",
    "    for line in data:\n",
    "        line = cleanse(line)\n",
    "        line = remove_retweet_prefix(line)\n",
    "        line = remove_at(line)\n",
    "        line = remove_hashtag(line)\n",
    "        line = remove_url(line)\n",
    "        match = pattern.search(line)\n",
    "\n",
    "        if match:\n",
    "            p = get_polarity(line)\n",
    "            doc = nlp(line)\n",
    "            \n",
    "            for ent in doc.ents:\n",
    "                if ent.label_ == 'PERSON':\n",
    "                    name = ent.text.strip()\n",
    "                    name = remove_apostrophe(name)\n",
    "                    # not add to list if the entity is null or golden globe related\n",
    "                    if p is None or name == '' or fuzz.ratio(name.lower(), 'golden globes') > 60:\n",
    "                        continue\n",
    "                    if p < 0:\n",
    "                        if name in neg_dict:\n",
    "                            neg_dict[name] += p\n",
    "                        else:\n",
    "                            neg_dict[name] = p\n",
    "                    \n",
    "                    if p > 0:\n",
    "                        if name in pos_dict:\n",
    "                            pos_dict[name] += p\n",
    "                        else:\n",
    "                            pos_dict[name] = p\n",
    "\n",
    "#     pos = {key: value for key, value in persons_dic.items() if value > 10}\n",
    "    # nominees = sorted(nominees.items(), key=lambda e: e[1], reverse=True)\n",
    "    \n",
    "    # get rid of names that are too long or too short\n",
    "    for name in list(pos_dict.keys()):\n",
    "        if len(name) < 5 or len(name) > 17:\n",
    "            del pos_dict[name]\n",
    "            \n",
    "    for name in list(neg_dict.keys()):\n",
    "        if len(name) < 5 or len(name) > 17:\n",
    "            del neg_dict[name]\n",
    "            \n",
    "    return pos_dict, neg_dict\n",
    "\n",
    "pos_dict, neg_dict = find_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Kate Hudson', 87.11540404040403),\n",
      " ('Lucy Liu', 40.93333333333334),\n",
      " ('Claire Danes', 39.35575396825397),\n",
      " ('Jennifer Lawrence', 38.71820226070226),\n",
      " ('Jessica Alba', 36.355),\n",
      " ('Anne Hathaway', 28.426388888888887),\n",
      " ('Jennifer Lopez', 24.532196969696955),\n",
      " ('Kerry Washington', 22.09953102453103),\n",
      " ('Nicole Kidman', 18.383333333333333),\n",
      " ('Jessica Chastain', 16.816849747474745)]\n",
      "list length: 403\n"
     ]
    }
   ],
   "source": [
    "# find the best dressed\n",
    "print_top(pos_dict, 10)\n",
    "\n",
    "# pprint.pprint(pos_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Sienna Miller', -14.383333333333333),\n",
      " ('Kate Hudson', -12.045833333333333),\n",
      " ('Alexander McQueen', -11.555555555555548),\n",
      " ('Lucy Liu', -10.37375541125541),\n",
      " ('Jennifer Lawrence', -9.366666666666667),\n",
      " ('Halle Berry', -7.711111111111111),\n",
      " ('Anne Hathaway', -4.796153846153847),\n",
      " ('Nicole Kidman', -4.666666666666666),\n",
      " ('Lena Dunham', -4.366666666666666),\n",
      " ('Lucy Lui', -2.6333333333333337)]\n",
      "145\n"
     ]
    }
   ],
   "source": [
    "# find the worst dressed\n",
    "sorted_neg = sorted(neg_dict.items(), key=lambda e: e[1], reverse=False)\n",
    "pprint.pprint(sorted_neg[0:10])\n",
    "print(len(sorted_neg))\n",
    "# pprint.pprint(neg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name_to_reduce(nominees):\n",
    "    names_clusters = []\n",
    "    names = list(nominees.keys())\n",
    "\n",
    "    for name in names:\n",
    "        # each name starts as a cluster\n",
    "        cluster = [name]\n",
    "        names_to_reduce = names[:]\n",
    "        names_to_reduce.remove(name)\n",
    "\n",
    "        # one vs. all comparisons\n",
    "        for i in names_to_reduce:\n",
    "            ratio = fuzz.ratio(name.lower(), i.lower())\n",
    "            # if similarity is larger than 75 or one name is contained in the other name\n",
    "            if ratio > 75 or re.search(name, i, flags=re.IGNORECASE) or re.search(i, name, flags=re.IGNORECASE):\n",
    "                cluster.append(i)\n",
    "\n",
    "        # if multiple names are identified in one cluster\n",
    "        if len(cluster) > 1:\n",
    "            names_clusters.append(cluster)\n",
    "\n",
    "    #     print(cluster)\n",
    "\n",
    "\n",
    "    # sort clusters\n",
    "    names_clusters.sort()\n",
    "    # sort within each cluster\n",
    "    names_clusters = ['|'.join(sorted(cluster)) for cluster in names_clusters]\n",
    "    # remove overlaps\n",
    "    names_clusters_reduced = [line.split('|') for line in list(set(names_clusters))]\n",
    "    # sort by length from shortest to longest (merge from the shortest)\n",
    "    names_clusters_reduced.sort(key=len)\n",
    "#     print('\\nnames clusters to merge:')\n",
    "#     pprint.pprint(names_clusters_reduced)\n",
    "#     print('\\n')\n",
    "    return names_clusters_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge names using cluster\n",
    "def reduce_names(nominees):\n",
    "    reduced_nominees = nominees.copy()\n",
    "    names_clusters_reduced = get_name_to_reduce(nominees)\n",
    "\n",
    "    def weighted_freq(element):\n",
    "        if element in reduced_nominees:\n",
    "            return abs(reduced_nominees[element]) * len(element)\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    for cluster in names_clusters_reduced:\n",
    "        # select the longest entity name\n",
    "        selected_entity_name = max(cluster, key=weighted_freq)\n",
    "        cluster.remove(selected_entity_name)\n",
    "        # for names to be merged to the selected entity name\n",
    "        for name in cluster:\n",
    "            # if not deleted in previous cases, cumulate frequencies to the selected entity\n",
    "            if name in reduced_nominees and selected_entity_name in reduced_nominees:\n",
    "                reduced_nominees[selected_entity_name] += reduced_nominees[name]\n",
    "                del reduced_nominees[name]\n",
    "    return reduced_nominees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Kate Hudson', 89.4320707070707),\n",
      " ('Jennifer Lawrence', 82.83838457838456),\n",
      " ('Lucy Liu', 53.15416666666666),\n",
      " ('Claire Danes', 42.780753968253975),\n",
      " ('Jessica Alba', 39.17999999999999),\n",
      " ('Nicole Kidman', 31.183333333333334),\n",
      " ('Anne Hathaway', 29.493055555555554),\n",
      " ('Julianne Moore', 24.17899711399711),\n",
      " ('Kerry Washington', 23.385245310245313),\n",
      " ('Jessica Chastain', 18.406849747474745),\n",
      " ('Eva Longoria', 13.41984126984127),\n",
      " ('Halle Berry', 13.160648148148146),\n",
      " ('Mermaid Dresses', 12.972727272727273),\n",
      " ('Taylor Swift', 12.831666666666665),\n",
      " ('Jodie Foster', 12.647619047619042),\n",
      " ('Bradley Cooper', 12.049999999999999),\n",
      " ('Miu Miu', 11.566666666666665),\n",
      " ('Emily Blunt', 10.802380952380954),\n",
      " ('Amy Poehler', 10.731406926406926),\n",
      " ('Lea Michele', 10.339444444444444)]\n",
      "list length: 265\n"
     ]
    }
   ],
   "source": [
    "# merge similar names, find top 20\n",
    "reduced_pos = reduce_names(pos_dict)\n",
    "print_top(reduced_pos, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Sienna Miller', -16.53333333333333),\n",
      " ('Lucy Liu', -14.807088744588741),\n",
      " ('Jennifer Lawrence', -13.76845238095238),\n",
      " ('Kate Hudson', -12.045833333333333),\n",
      " ('Alexander McQueen', -11.555555555555548),\n",
      " ('Halle Berry', -8.719444444444445),\n",
      " ('Anne Hathaway', -5.8961538461538465),\n",
      " ('Nicole Kidman', -4.666666666666666),\n",
      " ('Lena Dunham', -4.366666666666666),\n",
      " (\"Daniel Day Lewis'\", -2.9976190476190476),\n",
      " ('Jessica Chastain', -2.6025),\n",
      " ('Eva Longoria', -2.5263888888888886),\n",
      " ('PHOTOS', -2.45),\n",
      " ('Kristen Wiig', -2.386111111111111),\n",
      " ('Rosario Dawson', -2.35),\n",
      " ('Jodie Foster', -1.6714285714285713),\n",
      " ('Terrible Dress', -1.6666666666666667),\n",
      " ('Disappointed', -1.5),\n",
      " ('Amy Poehler', -1.3769345238095239),\n",
      " ('Guiliana', -1.2833333333333332)]\n",
      "104\n"
     ]
    }
   ],
   "source": [
    "# merge similar names, find top 20\n",
    "reduced_neg = reduce_names(neg_dict)\n",
    "sorted_neg = sorted(reduced_neg.items(), key=lambda e: e[1], reverse=False)\n",
    "pprint.pprint(sorted_neg[0:20])\n",
    "print(len(sorted_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Alexander McQueen': 33,\n",
      " 'Amanda Seyfried': 67,\n",
      " 'Amy Adams': 107,\n",
      " 'Amy Poehler': 36,\n",
      " 'Angelina Jolie': 293,\n",
      " 'Anne Hathaway': 12,\n",
      " 'Ben Affleck': 80,\n",
      " 'Bill Clinton': 67,\n",
      " 'Claire Danes': 23,\n",
      " 'Damien Lewis': 105,\n",
      " 'Debra Messing': 115,\n",
      " 'Don Cheadle': 179,\n",
      " 'Elie Saab': 196,\n",
      " 'Eva Longoria': 21,\n",
      " 'Halle Berry': 16,\n",
      " 'Heidi Klum': 209,\n",
      " 'Helen Hunt': 108,\n",
      " 'Helen Mirren': 85,\n",
      " 'Hugh Jackman': 159,\n",
      " 'Jennifer Lawrence': 3,\n",
      " 'Jessica Alba': 35,\n",
      " 'Jessica Chastain': 19,\n",
      " 'Jodie Foster': 29,\n",
      " 'Julianne Moore': 61,\n",
      " 'Kate Hudson': 3,\n",
      " 'Kelly Osbourne': 68,\n",
      " 'Kerry Washington': 69,\n",
      " 'Lena Dunham': 42,\n",
      " 'Looks': 167,\n",
      " 'Lucy Liu': 3,\n",
      " 'Marie Antoinette': 248,\n",
      " 'Megan Fox': 59,\n",
      " 'Michael Kors': 98,\n",
      " 'Naeem Khan': 90,\n",
      " 'Nicole Kidman': 12,\n",
      " 'Oscar': 148,\n",
      " 'PHOTOS': 45,\n",
      " 'Quentin Tarantino': 202,\n",
      " 'Rosario Dawson': 46,\n",
      " 'Sarah Hyland': 157,\n",
      " 'Sienna Miller': 39,\n",
      " 'Sofia Vergara': 60,\n",
      " 'Starbucks': 253,\n",
      " 'Tattoos': 275,\n",
      " 'Taylor Swift': 38,\n",
      " 'Tina Fey': 76}\n"
     ]
    }
   ],
   "source": [
    "# find the most controversial \n",
    "def get_sorted_names(nominees, f):\n",
    "    sorted_nominees = sorted(nominees.items(), key=lambda e: e[1], reverse=f)\n",
    "    names = [pair[0] for pair in sorted_nominees]\n",
    "    return names\n",
    "\n",
    "pos_names = get_sorted_names(reduced_pos, True)\n",
    "neg_names = get_sorted_names(reduced_neg, False)\n",
    "\n",
    "contro_dict = {}\n",
    "for name in pos_names:\n",
    "    if name in neg_names:\n",
    "        contro_dict[name] = pos_names.index(name) + neg_names.index(name)\n",
    "\n",
    "pprint.pprint(contro_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_python3",
   "language": "python",
   "name": "my_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
