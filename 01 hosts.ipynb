{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp_ms</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>290620657987887104</td>\n",
       "      <td>JLo's dress! #eredcarpet #GoldenGlobes</td>\n",
       "      <td>2013-01-14 00:45:38</td>\n",
       "      <td>{'screen_name': 'Dozaaa_xo', 'id': 557374298}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>290620657887219713</td>\n",
       "      <td>What's making Sofia Vergara's boobs stay like ...</td>\n",
       "      <td>2013-01-14 00:45:38</td>\n",
       "      <td>{'screen_name': 'theAmberShow', 'id': 14648726}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>290620657828524032</td>\n",
       "      <td>RT @FabSugar: Kerry Washington is EVERYTHING. ...</td>\n",
       "      <td>2013-01-14 00:45:38</td>\n",
       "      <td>{'screen_name': 'SweetyPW', 'id': 35498686}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id                                               text  \\\n",
       "0  290620657987887104             JLo's dress! #eredcarpet #GoldenGlobes   \n",
       "1  290620657887219713  What's making Sofia Vergara's boobs stay like ...   \n",
       "2  290620657828524032  RT @FabSugar: Kerry Washington is EVERYTHING. ...   \n",
       "\n",
       "         timestamp_ms                                             user  \n",
       "0 2013-01-14 00:45:38    {'screen_name': 'Dozaaa_xo', 'id': 557374298}  \n",
       "1 2013-01-14 00:45:38  {'screen_name': 'theAmberShow', 'id': 14648726}  \n",
       "2 2013-01-14 00:45:38      {'screen_name': 'SweetyPW', 'id': 35498686}  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(path_or_buf='gg2013.json')\n",
    "# df = pd.read_json(path_or_buf='gg2015.json')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0               JLo's dress! #eredcarpet #GoldenGlobes\n",
       "1    What's making Sofia Vergara's boobs stay like ...\n",
       "2    RT @FabSugar: Kerry Washington is EVERYTHING. ...\n",
       "3       Anne Hathaway has got me living. #GoldenGlobes\n",
       "4    Jennifer Lopez's lace dress? Thoughts? #Golden...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = df['text']\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "174643"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "174643"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample data if necessary\n",
    "sample_size = 200000\n",
    "if len(df) > sample_size:\n",
    "    data = data.sample(n=sample_size)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweets_freq_dict = {}\n",
    "\n",
    "def remove_retweet_prefix(line):\n",
    "    # find 'RT @abc: ' where abc's length is arbitrary\n",
    "    pattern = re.compile(r'\\bRT @([\\w\\'/]*)\\b: ') \n",
    "    match = re.search(pattern, line)\n",
    "    if match:\n",
    "        # store corresponding retweet without 'RT @' prefix\n",
    "        string = match.group()[4:]\n",
    "        if string in retweets_freq_dict:\n",
    "            retweets_freq_dict[string] += 1\n",
    "        else:\n",
    "            retweets_freq_dict[string] = 1\n",
    "    return re.sub(pattern, ' ', line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_freq_dict = {}\n",
    "\n",
    "def remove_hashtag(line):\n",
    "    pattern = re.compile(r'#([\\w\\'/]*)\\b')\n",
    "    matches = re.findall(pattern, line)\n",
    "    if matches:\n",
    "        # store corresponding hashtag\n",
    "        for match in matches:\n",
    "            if match in hashtag_freq_dict:\n",
    "                hashtag_freq_dict[match] += 1\n",
    "            else:\n",
    "                hashtag_freq_dict[match] = 1\n",
    "    return re.sub(pattern, ' ', line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "at_freq_dict = {}\n",
    "\n",
    "def remove_at(line):\n",
    "    pattern = re.compile(r'@([\\w\\'/]*)\\b')\n",
    "    matches = re.findall(pattern, line)\n",
    "    if matches:\n",
    "        # store corresponding hashtag\n",
    "        for match in matches:\n",
    "            if match in at_freq_dict:\n",
    "                at_freq_dict[match] += 1\n",
    "            else:\n",
    "                at_freq_dict[match] = 1\n",
    "    return re.sub(pattern, ' ', line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(line):\n",
    "    pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\\b')\n",
    "    return re.sub(pattern, ' ', line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanse(line):\n",
    "    # replace everything to ' ' except whitespace, alphanumeric character, apostrophe, hashtag, @\n",
    "    return re.sub(r'[^\\w\\s\\'#@]', ' ', line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_apostrophe(text):\n",
    "    pattern = re.compile(r'\\'s\\b')\n",
    "    return re.sub(pattern, ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# reference for token attributes\n",
    "# https://spacy.io/api/token#attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OMG   Adele  prepare to be the recipient of a very angry song from Taylor Swift #GoldenGlobes \n",
      "{'Adele  ': ['LANGUAGE'], 'Taylor Swift': ['PERSON']}\n",
      "Adele  \n"
     ]
    }
   ],
   "source": [
    "def identify_entities(text):\n",
    "    entities = list(nlp(text).ents)\n",
    "    tags = {}\n",
    "    for entity in entities:\n",
    "        if entity not in tags:\n",
    "#             tags[' '.join(t.orth_ for t in entity).strip()]=[entity.label_]\n",
    "#             print(entity.text)\n",
    "#             print(' '.join(t.orth_ for t in entity).strip())\n",
    "            tags[entity.text]=[entity.label_]\n",
    "    return tags\n",
    "\n",
    "# performance may be compared with nltk.tag.stanford.StanfordTagger if we have time\n",
    "# http://www.nltk.org/api/nltk.tag.html#module-nltk.tag.stanford\n",
    "\n",
    "# test\n",
    "text = cleanse(remove_retweet_prefix(data[53365]))\n",
    "tags = identify_entities(text)\n",
    "print(text)\n",
    "print(tags)\n",
    "print(list(tags.keys())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.9 s, sys: 98.6 ms, total: 3.99 s\n",
      "Wall time: 3.42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# generate cleansed_data beforehand\n",
    "cleansed_data = []\n",
    "for tweet in data:\n",
    "    # remove_retweet_prefix \n",
    "    line = remove_retweet_prefix(tweet)\n",
    "    # remove hashtag\n",
    "    line = remove_hashtag(line)\n",
    "    # remove @...\n",
    "    line = remove_at(line)\n",
    "    # remove url\n",
    "    line = remove_url(line)\n",
    "    # remove punctuations except apostrophe\n",
    "    line = cleanse(line)\n",
    "    cleansed_data.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"JLo's dress     \",\n",
       " \"What's making Sofia Vergara's boobs stay like that  Magic  Witchcraft   \",\n",
       " ' Kerry Washington is EVERYTHING  Dying over her Miu Miu gown       ',\n",
       " 'Anne Hathaway has got me living   ',\n",
       " \"Jennifer Lopez's lace dress  Thoughts   \"]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleansed_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def find_host(verbose=False):\n",
    "    pattern = re.compile(r'\\bhost')\n",
    "    entity_freq_dict = {}\n",
    "    \n",
    "    num = 0\n",
    "    max_entity_len = 0\n",
    "    max_entity = None\n",
    "    for line in cleansed_data:\n",
    "        match = re.search(pattern, line.lower())\n",
    "        if match:\n",
    "            tags = identify_entities(line)\n",
    "\n",
    "            if verbose:\n",
    "                # print the first 10 occurrences\n",
    "                if num < 10:\n",
    "                    print(tweet)\n",
    "                    print(line)\n",
    "                    print(tags)\n",
    "                    print()\n",
    "\n",
    "            for entity in tags.keys():\n",
    "                entity = entity.strip()\n",
    "                # identify the entity with maximum length\n",
    "                entity_len = len(entity)\n",
    "                if entity_len > max_entity_len:\n",
    "                    max_entity_len = entity_len\n",
    "                    max_entity = entity\n",
    "                \n",
    "                if entity_len > 1:\n",
    "                    if entity not in entity_freq_dict:\n",
    "                        entity_freq_dict[entity] = 1 # tried adding more weights to 'PERSON' tags but results are not good\n",
    "                    else:\n",
    "                        entity_freq_dict[entity] += 1\n",
    "            num += 1           \n",
    "    print('num of matches:', num)\n",
    "    print('max_entity_len:', max_entity_len)\n",
    "    print('max_entity:', max_entity)\n",
    "    return entity_freq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Says @BenAffleck: \"I also didn't get the Acting Nomination...no one's saying I got snubbed there!\" #pressroom #GoldenGlobes\n",
      "Looking forward to watching Tina Fey and Amy Poehler host the  \n",
      "{'Tina Fey': ['EVENT'], 'Amy Poehler': ['PERSON']}\n",
      "\n",
      "Says @BenAffleck: \"I also didn't get the Acting Nomination...no one's saying I got snubbed there!\" #pressroom #GoldenGlobes\n",
      " It's our hosts Tina Fey and Amy Poehler       \n",
      "{'Tina Fey': ['PERSON'], 'Amy Poehler       ': ['PERSON']}\n",
      "\n",
      "Says @BenAffleck: \"I also didn't get the Acting Nomination...no one's saying I got snubbed there!\" #pressroom #GoldenGlobes\n",
      " Tonight's dual hosting duties represent the culmination of a decade of Amy and Tina partnerships     \n",
      "{'Tonight': ['TIME'], 'a decade': ['DATE'], 'Amy': ['PERSON'], 'Tina': ['GPE']}\n",
      "\n",
      "Says @BenAffleck: \"I also didn't get the Acting Nomination...no one's saying I got snubbed there!\" #pressroom #GoldenGlobes\n",
      " My green suede tuxedo pinching a bit here at the Velvet Rope Awards honoring best in crowd control  Topo Gigio   I hosting  \n",
      "{' ': ['NORP'], 'Topo Gigio': ['PERSON']}\n",
      "\n",
      "Says @BenAffleck: \"I also didn't get the Acting Nomination...no one's saying I got snubbed there!\" #pressroom #GoldenGlobes\n",
      " Tina Fey  amp  Amy Poehler Talk   Hosting  Drinking Game   \n",
      "{' Tina Fey  ': ['ORG'], 'Amy': ['PERSON'], 'Hosting': ['GPE']}\n",
      "\n",
      "Says @BenAffleck: \"I also didn't get the Acting Nomination...no one's saying I got snubbed there!\" #pressroom #GoldenGlobes\n",
      "  Best choice for host ever   Nice job GG people \n",
      "{'  Nice': ['LOC']}\n",
      "\n",
      "Says @BenAffleck: \"I also didn't get the Acting Nomination...no one's saying I got snubbed there!\" #pressroom #GoldenGlobes\n",
      " We re going to keep things loose   said Amy Poehler of her and co host Tina Fey s plan for the evening     \n",
      "{'  ': ['ORG'], 'Amy Poehler': ['PERSON'], 'Tina Fey s': ['PERSON'], 'the evening     ': ['TIME']}\n",
      "\n",
      "Says @BenAffleck: \"I also didn't get the Acting Nomination...no one's saying I got snubbed there!\" #pressroom #GoldenGlobes\n",
      "   hosts Tina Fey  Amy Poehler show off matching  husband and wife  outfits on red carpet  \n",
      "{'Tina Fey': ['ORG'], 'Amy Poehler': ['PERSON'], ' ': ['NORP']}\n",
      "\n",
      "Says @BenAffleck: \"I also didn't get the Acting Nomination...no one's saying I got snubbed there!\" #pressroom #GoldenGlobes\n",
      "   hosts Tina Fey  Amy Poehler show off matching  husband and wife  outfits on red carpet  \n",
      "{'Tina Fey': ['ORG'], 'Amy Poehler': ['PERSON'], ' ': ['NORP']}\n",
      "\n",
      "Says @BenAffleck: \"I also didn't get the Acting Nomination...no one's saying I got snubbed there!\" #pressroom #GoldenGlobes\n",
      " Tonight's dual hosting duties represent the culmination of a decade of Amy and Tina partnerships     \n",
      "{'Tonight': ['TIME'], 'a decade': ['DATE'], 'Amy': ['PERSON'], 'Tina': ['GPE']}\n",
      "\n",
      "num of matches: 2927\n",
      "max_entity_len: 38\n",
      "max_entity: THE FUCKING HOSTS OF THE GOLDEN GLOBES\n",
      "CPU times: user 1min 55s, sys: 20.1 s, total: 2min 15s\n",
      "Wall time: 23.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# note that for 2015 tweets, it takes more than 4min to run\n",
    "# processing time may be long for large data\n",
    "# consider picking the longest 150000 tweets\n",
    "\n",
    "entity_freq_dict = find_host(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('goldenglobes: ', 3459),\n",
       " ('eonline: ', 2891),\n",
       " ('PerezHilton: ', 2301),\n",
       " ('TheEllenShow: ', 1696),\n",
       " ('EmWatson: ', 1147),\n",
       " ('VanityFair: ', 820),\n",
       " ('nbcsnl: ', 743),\n",
       " ('CNNshowbiz: ', 699),\n",
       " ('CiudadBizarra: ', 510),\n",
       " ('BuzzFeed: ', 491),\n",
       " ('EW: ', 485),\n",
       " ('nbc: ', 465),\n",
       " ('vulture: ', 446),\n",
       " ('piersmorgan: ', 441),\n",
       " ('MARLONLWAYANS: ', 384),\n",
       " ('HuffingtonPost: ', 369),\n",
       " ('buckhollywood: ', 353),\n",
       " ('MarilynMonroeES: ', 334),\n",
       " ('TVGuide: ', 319),\n",
       " ('THR: ', 315),\n",
       " ('DavidSpade: ', 312),\n",
       " ('MTVNews: ', 307),\n",
       " ('PimpBillClinton: ', 281),\n",
       " ('cinema21: ', 281),\n",
       " ('washingtonpost: ', 275),\n",
       " ('ninagarcia: ', 258),\n",
       " ('Cosmopolitan: ', 239),\n",
       " ('peopleenespanol: ', 219),\n",
       " ('RichardCrouse: ', 217),\n",
       " ('peoplemag: ', 213),\n",
       " ('prodigalsam: ', 212),\n",
       " ('kumailn: ', 211),\n",
       " ('DannyZuker: ', 207),\n",
       " ('jianghomeshi: ', 201),\n",
       " ('HuffPostWomen: ', 200),\n",
       " ('DamienFahey: ', 200),\n",
       " ('girlsHBO: ', 198),\n",
       " ('MHarrisPerry: ', 194),\n",
       " ('DougBenson: ', 192),\n",
       " ('rogergzz: ', 191),\n",
       " ('InStyle: ', 191),\n",
       " ('EonlineLatino: ', 188),\n",
       " ('TVWithoutPity: ', 187),\n",
       " ('glamourmag: ', 181),\n",
       " ('CinePREMIERE: ', 174),\n",
       " ('unfoRETTAble: ', 173),\n",
       " ('HuffPostEnt: ', 167),\n",
       " ('msleamichele: ', 164),\n",
       " ('SofiaVergara: ', 160),\n",
       " ('heidiklum: ', 158)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top retweets (each as an entity)\n",
    "sorted(retweets_freq_dict.items(), key=lambda pair: pair[1], reverse=True)[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('GoldenGlobes', 110135),\n",
       " ('goldenglobes', 33113),\n",
       " ('Argo', 842),\n",
       " ('GetGlue', 769),\n",
       " ('Homeland', 696),\n",
       " ('redcarpet', 675),\n",
       " ('GoldenGlobe', 571),\n",
       " ('Goldenglobes', 560),\n",
       " ('JodieFoster', 486),\n",
       " ('Girls', 451),\n",
       " ('killingit', 447),\n",
       " ('GOLDENGLOBES', 420),\n",
       " ('AlfombraRojaE', 385),\n",
       " ('Skyfall', 360),\n",
       " ('GlobosdeOro', 352),\n",
       " ('RedCarpet', 348),\n",
       " ('Lincoln', 348),\n",
       " ('LesMis', 346),\n",
       " ('GIRLS', 332),\n",
       " ('Adele', 289),\n",
       " ('LesMiserables', 282),\n",
       " ('JenniferLawrence', 258),\n",
       " ('GG2013', 254),\n",
       " ('homeland', 251),\n",
       " ('GoldenGlobes2013', 249),\n",
       " ('TinaFey', 226),\n",
       " ('ERedCarpet', 224),\n",
       " ('AmyPoehler', 209),\n",
       " ('eredcarpet', 207),\n",
       " ('jodiefoster', 204),\n",
       " ('DowntonAbbey', 193),\n",
       " ('AnneHathaway', 191),\n",
       " ('girls', 188),\n",
       " ('DjangoUnchained', 184),\n",
       " ('Oscars', 176),\n",
       " ('LoveThoseLadies', 163),\n",
       " ('Django', 158),\n",
       " ('BenAffleck', 154),\n",
       " ('Globes', 149),\n",
       " ('GameChange', 146),\n",
       " ('LOVEHER', 146),\n",
       " ('goldenGlobes', 143),\n",
       " ('fb', 143),\n",
       " ('BillClinton', 142),\n",
       " ('fashion', 138),\n",
       " ('Oscar', 138),\n",
       " ('skyfall', 137),\n",
       " ('watchyourbackAdele', 133),\n",
       " ('adele', 130),\n",
       " ('argo', 128)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top hashtags\n",
    "sorted(hashtag_freq_dict.items(), key=lambda pair: pair[1], reverse=True)[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('goldenglobes', 3150),\n",
       " ('OfficialAdele', 1122),\n",
       " ('lenadunham', 980),\n",
       " ('GoldenGlobes', 863),\n",
       " ('BenAffleck', 526),\n",
       " ('RealHughJackman', 386),\n",
       " ('PerezHilton', 361),\n",
       " ('SHO_Homeland', 357),\n",
       " ('girlsHBO', 352),\n",
       " ('SofiaVergara', 303),\n",
       " ('eonline', 290),\n",
       " ('taylorswift13', 270),\n",
       " ('TNTLA', 255),\n",
       " ('JLo', 235),\n",
       " ('PaulEpworth', 223),\n",
       " ('GirlsHBO', 182),\n",
       " ('msleamichele', 167),\n",
       " ('SelenaGomez', 164),\n",
       " ('VanessuHudgens', 163),\n",
       " ('LenaDunham', 156),\n",
       " ('steph_hart', 156),\n",
       " ('RyanGosling', 137),\n",
       " ('kerrywashington', 125),\n",
       " ('PixarBrave', 120),\n",
       " ('LesMiserables', 113),\n",
       " ('nbc', 112),\n",
       " ('Chanel', 112),\n",
       " ('NathanFillion', 105),\n",
       " ('KevalBaxi', 97),\n",
       " ('Burberry', 86),\n",
       " ('Lewis_Damian', 86),\n",
       " ('jessicaalba', 84),\n",
       " ('CHANEL', 83),\n",
       " ('tomandlorenzo', 78),\n",
       " ('nbcsnl', 77),\n",
       " ('iamdoncheadle', 76),\n",
       " ('piersmorgan', 69),\n",
       " ('CiudadBizarra', 67),\n",
       " ('CNNshowbiz', 64),\n",
       " ('BuzzFeed', 64),\n",
       " ('LeoDiCaprio', 62),\n",
       " ('EvaLongoria', 61),\n",
       " ('azizansari', 60),\n",
       " ('gabrielledoug', 60),\n",
       " ('Sarah_Hyland', 60),\n",
       " (\"lenadunham's\", 58),\n",
       " ('WorldMcQueen', 55),\n",
       " ('EW', 55),\n",
       " ('Solvej_Schou', 55),\n",
       " ('officialadele', 54)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top @\n",
    "sorted(at_freq_dict.items(), key=lambda pair: pair[1], reverse=True)[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Will Ferrell', 700),\n",
       " ('Amy Poehler', 604),\n",
       " ('Amy', 529),\n",
       " ('Kristen Wiig', 422),\n",
       " ('Tina', 418),\n",
       " ('next year', 377),\n",
       " ('Tina Fey', 352),\n",
       " ('Oscars next year', 258),\n",
       " ('2014', 165),\n",
       " ('Golden Globes', 99),\n",
       " ('Quentin Tarantino', 95),\n",
       " ('Best  Hosts', 89),\n",
       " ('Scratch', 62),\n",
       " ('the Golden Globes', 58),\n",
       " ('Opening Monologue', 54),\n",
       " ('Paul Rudd', 53),\n",
       " ('two', 50),\n",
       " ('Golden Globes Hosts Tina', 50),\n",
       " ('Amy Poelher', 48),\n",
       " ('SNL', 47),\n",
       " ('Thoroughly', 46),\n",
       " ('tonight', 43),\n",
       " ('Great', 42),\n",
       " ('Can Tina', 42),\n",
       " (\"next year's\", 41),\n",
       " ('RT', 37),\n",
       " ('this Golden Globes  Tina', 37),\n",
       " ('Iran', 30),\n",
       " ('Hollywood', 29),\n",
       " ('Please', 29),\n",
       " ('Academy', 28),\n",
       " ('every year', 25),\n",
       " ('Kristen Wiig and', 25),\n",
       " ('Canadians', 25),\n",
       " ('Canada', 25),\n",
       " ('Amy Poehler on Hosting Golden Globes', 23),\n",
       " ('Poehler', 22),\n",
       " ('this year', 21),\n",
       " ('Host', 21),\n",
       " ('Amy Pohler', 20),\n",
       " ('first', 19),\n",
       " ('Can Tina Fey', 18),\n",
       " ('Love', 17),\n",
       " ('2013', 16),\n",
       " ('So', 16),\n",
       " (\"Amy Poehler's\", 15),\n",
       " ('Oscars', 15),\n",
       " ('Run the World', 15),\n",
       " ('Wiig', 15),\n",
       " ('Quentin Tarantin', 15),\n",
       " ('next years', 14),\n",
       " ('Ricky Gervais', 13),\n",
       " ('Can', 13),\n",
       " ('Ferrell', 13),\n",
       " ('Cohen', 13),\n",
       " ('one', 12),\n",
       " ('70th', 11),\n",
       " ('Bottoms up', 11),\n",
       " ('Kristin Wiig', 11),\n",
       " ('Hosts Tina Fey', 10),\n",
       " ('NBC', 9),\n",
       " ('Adele', 9),\n",
       " ('Kristen Wig', 9),\n",
       " ('Lorne Michaels', 9),\n",
       " ('Helena Bonham Carter', 9),\n",
       " ('SNL Lorne', 9),\n",
       " ('Robert Downey Jr', 9),\n",
       " ('Tonight', 8),\n",
       " ('James Cameron', 8),\n",
       " ('year', 8),\n",
       " ('AMY', 7),\n",
       " ('Oscar', 7),\n",
       " ('Maggie Smith', 7),\n",
       " ('Best', 7),\n",
       " ('Jennifer Lawrence', 7),\n",
       " ('Homeland', 7),\n",
       " ('Maggie Smi', 7),\n",
       " ('Will Ferrel', 7),\n",
       " ('Kristin', 7),\n",
       " (\"George Clooney's\", 7),\n",
       " ('Faye', 7),\n",
       " ('Jodie Foster', 7),\n",
       " ('Jay Leno', 6),\n",
       " ('Tina Amy', 6),\n",
       " ('Bill Clinton', 6),\n",
       " ('Will', 6),\n",
       " ('Will Farrell', 6),\n",
       " ('Hosts', 6),\n",
       " ('hosts', 6),\n",
       " ('Les Miles', 6),\n",
       " ('Sacha', 6),\n",
       " ('Sasha', 6),\n",
       " ('Christian Bale', 6),\n",
       " ('Iron Man', 6),\n",
       " ('Bruce Wayne', 6),\n",
       " ('Battle Of The', 6),\n",
       " ('Billionaires', 6),\n",
       " ('a decade', 5),\n",
       " ('The Golden Globes', 5),\n",
       " ('James Franco', 5)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_100 = sorted(entity_freq_dict.items(), key=lambda pair: pair[1], reverse=True)[:100]\n",
    "top_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67\n",
      "43\n",
      "85\n"
     ]
    }
   ],
   "source": [
    "# pip3 install python-Levenshtein for 4-10x speedup\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# test\n",
    "print(fuzz.ratio('Tina Fey', 'Tina'))\n",
    "print(fuzz.ratio('Amy Poehler', 'Amy'))\n",
    "print(fuzz.ratio('Golden Globes', 'golden globes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Golden Globes',\n",
      " 'the Golden Globes',\n",
      " 'Golden Globes Hosts Tina',\n",
      " 'this Golden Globes  Tina',\n",
      " 'The Golden Globes']\n"
     ]
    }
   ],
   "source": [
    "# remove golden globes from names\n",
    "import pprint\n",
    "names = [pair[0] for pair in top_100]\n",
    "golden_globes = [name for name in names if fuzz.ratio(name.lower(), 'golden globes') > 60]\n",
    "pprint.pprint(golden_globes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we also consider dropping all lower cases examples or examples that contain digit(s), which are not names\n",
    "def filter_names(pair_list):\n",
    "    filtered_results = []\n",
    "    for pair in pair_list:\n",
    "        string = ''.join(pair[0].split())\n",
    "        if not all(char.islower() for char in string) and not any(char.isdigit() for char in string):\n",
    "            filtered_results.append(pair)\n",
    "        else:\n",
    "            if pair[0] in entity_freq_dict:\n",
    "                del entity_freq_dict[pair[0]]\n",
    "    return filtered_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Will Ferrell', 700),\n",
       " ('Amy Poehler', 604),\n",
       " ('Amy', 529),\n",
       " ('Kristen Wiig', 422),\n",
       " ('Tina', 418),\n",
       " ('Tina Fey', 352),\n",
       " ('Oscars next year', 258),\n",
       " ('Quentin Tarantino', 95),\n",
       " ('Best  Hosts', 89),\n",
       " ('Scratch', 62),\n",
       " ('Opening Monologue', 54),\n",
       " ('Paul Rudd', 53),\n",
       " ('Amy Poelher', 48),\n",
       " ('SNL', 47),\n",
       " ('Thoroughly', 46),\n",
       " ('Great', 42),\n",
       " ('Can Tina', 42),\n",
       " (\"next year's\", 41),\n",
       " ('RT', 37),\n",
       " ('Iran', 30),\n",
       " ('Hollywood', 29),\n",
       " ('Please', 29),\n",
       " ('Academy', 28),\n",
       " ('Kristen Wiig and', 25),\n",
       " ('Canadians', 25),\n",
       " ('Canada', 25),\n",
       " ('Amy Poehler on Hosting Golden Globes', 23),\n",
       " ('Poehler', 22),\n",
       " ('Host', 21),\n",
       " ('Amy Pohler', 20),\n",
       " ('Can Tina Fey', 18),\n",
       " ('Love', 17),\n",
       " ('So', 16),\n",
       " (\"Amy Poehler's\", 15),\n",
       " ('Oscars', 15),\n",
       " ('Run the World', 15),\n",
       " ('Wiig', 15),\n",
       " ('Quentin Tarantin', 15),\n",
       " ('Ricky Gervais', 13),\n",
       " ('Can', 13),\n",
       " ('Ferrell', 13)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for name in golden_globes:\n",
    "    if name in entity_freq_dict:\n",
    "        del entity_freq_dict[name]\n",
    "top_results = sorted(entity_freq_dict.items(), key=lambda pair: pair[1], reverse=True)[:50]\n",
    "top_results = filter_names(top_results)\n",
    "top_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Will Ferrell', 'Ferrell']\n",
      "['Amy Poehler', 'Amy', 'Amy Poelher', 'Amy Poehler on Hosting Golden Globes', 'Poehler', 'Amy Pohler', \"Amy Poehler's\"]\n",
      "['Amy', 'Amy Poehler', 'Amy Poelher', 'Amy Poehler on Hosting Golden Globes', 'Amy Pohler', \"Amy Poehler's\"]\n",
      "['Kristen Wiig', 'Kristen Wiig and', 'Wiig']\n",
      "['Tina', 'Tina Fey', 'Can Tina', 'Can Tina Fey']\n",
      "['Tina Fey', 'Tina', 'Can Tina Fey']\n",
      "['Oscars next year', 'Oscars']\n",
      "['Quentin Tarantino', 'Quentin Tarantin']\n",
      "['Best  Hosts', 'Host']\n",
      "['Scratch']\n",
      "['Opening Monologue']\n",
      "['Paul Rudd']\n",
      "['Amy Poelher', 'Amy Poehler', 'Amy', 'Amy Pohler', \"Amy Poehler's\"]\n",
      "['SNL']\n",
      "['Thoroughly']\n",
      "['Great']\n",
      "['Can Tina', 'Tina', 'Can Tina Fey', 'Can']\n",
      "[\"next year's\"]\n",
      "['RT']\n",
      "['Iran']\n",
      "['Hollywood']\n",
      "['Please']\n",
      "['Academy']\n",
      "['Kristen Wiig and', 'Kristen Wiig', 'Wiig']\n",
      "['Canadians', 'Canada', 'Can']\n",
      "['Canada', 'Canadians', 'Can']\n",
      "['Amy Poehler on Hosting Golden Globes', 'Amy Poehler', 'Amy', 'Poehler', 'Host']\n",
      "['Poehler', 'Amy Poehler', 'Amy Poehler on Hosting Golden Globes', \"Amy Poehler's\"]\n",
      "['Host', 'Best  Hosts', 'Amy Poehler on Hosting Golden Globes']\n",
      "['Amy Pohler', 'Amy Poehler', 'Amy', 'Amy Poelher', \"Amy Poehler's\"]\n",
      "['Can Tina Fey', 'Tina', 'Tina Fey', 'Can Tina', 'Can']\n",
      "['Love']\n",
      "['So']\n",
      "[\"Amy Poehler's\", 'Amy Poehler', 'Amy', 'Amy Poelher', 'Poehler', 'Amy Pohler']\n",
      "['Oscars', 'Oscars next year']\n",
      "['Run the World']\n",
      "['Wiig', 'Kristen Wiig', 'Kristen Wiig and']\n",
      "['Quentin Tarantin', 'Quentin Tarantino']\n",
      "['Ricky Gervais']\n",
      "['Can', 'Can Tina', 'Canadians', 'Canada', 'Can Tina Fey']\n",
      "['Ferrell', 'Will Ferrell']\n",
      "\n",
      "names clusters to merge:\n",
      "[['Quentin Tarantin', 'Quentin Tarantino'],\n",
      " ['Best  Hosts', 'Host'],\n",
      " ['Ferrell', 'Will Ferrell'],\n",
      " ['Oscars', 'Oscars next year'],\n",
      " ['Kristen Wiig', 'Kristen Wiig and', 'Wiig'],\n",
      " ['Amy Poehler on Hosting Golden Globes', 'Best  Hosts', 'Host'],\n",
      " ['Can Tina Fey', 'Tina', 'Tina Fey'],\n",
      " ['Can', 'Canada', 'Canadians'],\n",
      " ['Can', 'Can Tina', 'Can Tina Fey', 'Tina'],\n",
      " ['Amy Poehler',\n",
      "  'Amy Poehler on Hosting Golden Globes',\n",
      "  \"Amy Poehler's\",\n",
      "  'Poehler'],\n",
      " ['Can Tina', 'Can Tina Fey', 'Tina', 'Tina Fey'],\n",
      " ['Can', 'Can Tina', 'Can Tina Fey', 'Tina', 'Tina Fey'],\n",
      " ['Can', 'Can Tina', 'Can Tina Fey', 'Canada', 'Canadians'],\n",
      " ['Amy',\n",
      "  'Amy Poehler',\n",
      "  'Amy Poehler on Hosting Golden Globes',\n",
      "  'Host',\n",
      "  'Poehler'],\n",
      " ['Amy', 'Amy Poehler', \"Amy Poehler's\", 'Amy Poelher', 'Amy Pohler'],\n",
      " ['Amy',\n",
      "  'Amy Poehler',\n",
      "  'Amy Poehler on Hosting Golden Globes',\n",
      "  \"Amy Poehler's\",\n",
      "  'Amy Poelher',\n",
      "  'Amy Pohler'],\n",
      " ['Amy',\n",
      "  'Amy Poehler',\n",
      "  \"Amy Poehler's\",\n",
      "  'Amy Poelher',\n",
      "  'Amy Pohler',\n",
      "  'Poehler'],\n",
      " ['Amy',\n",
      "  'Amy Poehler',\n",
      "  'Amy Poehler on Hosting Golden Globes',\n",
      "  \"Amy Poehler's\",\n",
      "  'Amy Poelher',\n",
      "  'Amy Pohler',\n",
      "  'Poehler']]\n"
     ]
    }
   ],
   "source": [
    "names = [pair[0] for pair in top_results]\n",
    "names_clusters = []\n",
    "\n",
    "for name in names:\n",
    "    # each name starts as a cluster\n",
    "    cluster = [name]\n",
    "    names_to_reduce = names[:]\n",
    "    names_to_reduce.remove(name)\n",
    "    \n",
    "    # one vs. all comparisons\n",
    "    for i in names_to_reduce:\n",
    "        ratio = fuzz.ratio(name.lower(), i.lower())\n",
    "        # if similarity is larger than 75 or one name is contained in the other name\n",
    "        if ratio > 75 or re.search(name, i, flags=re.IGNORECASE) or re.search(i, name, flags=re.IGNORECASE):\n",
    "            cluster.append(i)\n",
    "    \n",
    "    # if multiple names are identified in one cluster\n",
    "    if len(cluster) > 1:\n",
    "        names_clusters.append(cluster)\n",
    "    \n",
    "    print(cluster)\n",
    "\n",
    "# find names clusters that should merge\n",
    "# ['Amy Poehler', 'Amy', 'Amy Poelher']\n",
    "# ['Tina', 'Tina Fey']\n",
    "\n",
    "# sort clusters\n",
    "names_clusters.sort()\n",
    "# sort within each cluster\n",
    "names_clusters = ['|'.join(sorted(cluster)) for cluster in names_clusters]\n",
    "# remove overlaps\n",
    "names_clusters_reduced = [line.split('|') for line in list(set(names_clusters))]\n",
    "# sort by length from shortest to longest (merge from the shortest)\n",
    "names_clusters_reduced.sort(key=len)\n",
    "print('\\nnames clusters to merge:')\n",
    "pprint.pprint(names_clusters_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# weighted frequency of an entity is defined by its frequency multiplied by its string length\n",
    "def weighted_freq(element):\n",
    "    return entity_freq_dict[element] * len(element)\n",
    "\n",
    "e = entity_freq_dict.copy()\n",
    "for cluster in names_clusters_reduced:\n",
    "    # select the entity name with highest weighted frequency\n",
    "    selected_entity_name = max(cluster, key=weighted_freq)\n",
    "    cluster.remove(selected_entity_name)\n",
    "    # for names to be merged to the selected entity name\n",
    "    for name in cluster:\n",
    "        # if not deleted in previous cases, cumulate frequencies to the selected entity\n",
    "        if name in e and selected_entity_name in e:\n",
    "            e[selected_entity_name] += e[name]\n",
    "            del e[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Amy Poehler', 1238),\n",
       " ('Tina Fey', 830),\n",
       " ('Will Ferrell', 713),\n",
       " ('Kristen Wiig', 462),\n",
       " ('Oscars next year', 273),\n",
       " ('Best  Hosts', 133),\n",
       " ('Quentin Tarantino', 110),\n",
       " ('Canadians', 63),\n",
       " ('Scratch', 62),\n",
       " ('Opening Monologue', 54)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_10 = sorted(e.items(), key=lambda pair: pair[1], reverse=True)[:10]\n",
    "top_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"Host\": [\"Amy Poehler\", \"Tina Fey\"]}'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# top 2 inferences for hosts\n",
    "best_host_prediction = [name[0] for name in top_10][:2]\n",
    "json.dumps({'Host': best_host_prediction})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
